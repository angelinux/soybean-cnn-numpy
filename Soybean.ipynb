{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Soybean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelinux/soybean-cnn-numpy/blob/master/Soybean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkobP-baP94S",
        "colab_type": "text"
      },
      "source": [
        "# Soybean CNN Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3PyA-0ZQHvg",
        "colab_type": "text"
      },
      "source": [
        "## Gathering data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTJQcfWH8KSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d fpeccia/weed-detection-in-soybean-crops\n",
        "!rm -rf dataset\n",
        "!unzip weed-detection-in-soybean-crops.zip\n",
        "!rm -rf dataset/dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcAJMcMg_0kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from random import shuffle\n",
        "from random import randint\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbFuSOLO8yvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sz = 64\n",
        "classes = os.listdir('dataset')\n",
        "n_filters = 8\n",
        "\n",
        "old_prefix = 'dataset/'\n",
        "new_prefix = 'dataset-transf/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd0nNPwQBWT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_image(image_name):\n",
        "  image = cv2.imread(image_name, cv2.IMREAD_UNCHANGED)\n",
        "  image = cv2.resize(image, (sz,sz))\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slINvQAS_XX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.rmtree(new_prefix, ignore_errors=True)\n",
        "os.mkdir(new_prefix)\n",
        "for i in range(len(classes)):\n",
        "  os.mkdir(new_prefix + classes[i])\n",
        "  for f in os.listdir(old_prefix + classes[i]):\n",
        "    image = transform_image(os.path.join(old_prefix+classes[i], f))\n",
        "    cv2.imwrite(os.path.join(new_prefix+classes[i], f), image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NqlSJmf1BRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alldata = list()\n",
        "for i in range(len(classes)):\n",
        "  for f in os.listdir(new_prefix+classes[i]):\n",
        "    alldata.append([cv2.imread(os.path.join(new_prefix+classes[i], f), cv2.IMREAD_UNCHANGED), i])\n",
        "\n",
        "data_train, data_test = train_test_split(alldata, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xuIIxP9bN_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86886ed4-d74e-417e-e723-a5b156f8eb89"
      },
      "source": [
        "print(len(data_train))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keCJkpIeQN-j",
        "colab_type": "text"
      },
      "source": [
        "## CNN component definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC0RYRMTB7I1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Convolution3x3:\n",
        "  #class for convolution with 3x3 filters\n",
        "  \n",
        "  def __init__(self, num_filters):\n",
        "    self.num_filters = num_filters\n",
        "\n",
        "    # filters is a 3d array with dimensions (num_filters, 3, 3)\n",
        "    # We divide by 9 to reduce the variance of our initial values\n",
        "    self.filters = np.random.randn(num_filters, 3, 3) / 9    \n",
        "\n",
        "  def iterate_regions(self, image):\n",
        "    '''\n",
        "    Generates all possible 3x3 image regions using valid padding.\n",
        "    - image is a 2d numpy array.\n",
        "    '''\n",
        "    h, w = image.shape\n",
        "\n",
        "    for i in range(h - 2):\n",
        "      for j in range(w - 2):\n",
        "        im_region = image[i:(i + 3), j:(j + 3)]\n",
        "        yield im_region, i, j        \n",
        "\n",
        "  def forward(self, input):\n",
        "    '''\n",
        "    Performs a forward pass of the conv layer using the given input.\n",
        "    Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
        "    - input is a 2d numpy array\n",
        "    '''\n",
        "    self.last_input = input\n",
        "\n",
        "    h, w = input.shape\n",
        "    output = np.zeros((h - 2, w - 2, self.num_filters))\n",
        "\n",
        "    for im_region, i, j in self.iterate_regions(input):\n",
        "      output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
        "\n",
        "    return output \n",
        "  \n",
        "\n",
        "  def backprop(self, d_L_d_out, learn_rate):\n",
        "    '''\n",
        "    Performs a backward pass of the conv layer.\n",
        "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
        "    - learn_rate is a float.\n",
        "    '''\n",
        "    d_L_d_filters = np.zeros(self.filters.shape)\n",
        "\n",
        "    for im_region, i, j in self.iterate_regions(self.last_input):\n",
        "      for f in range(self.num_filters):\n",
        "        d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n",
        "\n",
        "    # Update filters\n",
        "    self.filters -= learn_rate * d_L_d_filters\n",
        "\n",
        "    # We aren't returning anything here since we use Conv3x3 as the first layer in our CNN.\n",
        "    # Otherwise, we'd need to return the loss gradient for this layer's inputs, just like every\n",
        "    # other layer in our CNN.\n",
        "    return None\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7tm8kbAEny_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaxPool2:\n",
        "  # A Max Pooling layer using a pool size of 2.\n",
        "\n",
        "  def iterate_regions(self, image):\n",
        "    '''\n",
        "    Generates non-overlapping 2x2 image regions to pool over.\n",
        "    - image is a 2d numpy array\n",
        "    '''\n",
        "    h, w, _ = image.shape\n",
        "    new_h = h // 2\n",
        "    new_w = w // 2\n",
        "\n",
        "    for i in range(new_h):\n",
        "      for j in range(new_w):\n",
        "        im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
        "        yield im_region, i, j\n",
        "\n",
        "  def forward(self, input):\n",
        "    '''\n",
        "    Performs a forward pass of the maxpool layer using the given input.\n",
        "    Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
        "    - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
        "    '''\n",
        "    self.last_input = input\n",
        "\n",
        "    h, w, num_filters = input.shape\n",
        "    output = np.zeros((h // 2, w // 2, num_filters))\n",
        "\n",
        "    for im_region, i, j in self.iterate_regions(input):\n",
        "      output[i, j] = np.amax(im_region, axis=(0, 1))\n",
        "\n",
        "    return output\n",
        "\n",
        "  def backprop(self, d_L_d_out):\n",
        "    '''\n",
        "    Performs a backward pass of the maxpool layer.\n",
        "    Returns the loss gradient for this layer's inputs.\n",
        "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
        "    '''\n",
        "    d_L_d_input = np.zeros(self.last_input.shape)\n",
        "\n",
        "    for im_region, i, j in self.iterate_regions(self.last_input):\n",
        "      h, w, f = im_region.shape\n",
        "      amax = np.amax(im_region, axis=(0, 1))\n",
        "\n",
        "      for i2 in range(h):\n",
        "        for j2 in range(w):\n",
        "          for f2 in range(f):\n",
        "            # If this pixel was the max value, copy the gradient to it.\n",
        "            if im_region[i2, j2, f2] == amax[f2]:\n",
        "              d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
        "\n",
        "    return d_L_d_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44aFGVdzFvKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Softmax:\n",
        "  # A standard fully-connected layer with softmax activation.\n",
        "\n",
        "  def __init__(self, input_len, nodes):\n",
        "    # We divide by input_len to reduce the variance of our initial values\n",
        "    self.weights = np.random.randn(input_len, nodes) / input_len\n",
        "    self.biases = np.zeros(nodes)\n",
        "\n",
        "  def forward(self, input):\n",
        "    '''\n",
        "    Performs a forward pass of the softmax layer using the given input.\n",
        "    Returns a 1d numpy array containing the respective probability values.\n",
        "    - input can be any array with any dimensions.\n",
        "    '''\n",
        "    self.last_input_shape = input.shape\n",
        "\n",
        "    input = input.flatten()\n",
        "    self.last_input = input\n",
        "\n",
        "    input_len, nodes = self.weights.shape\n",
        "\n",
        "    totals = np.dot(input, self.weights) + self.biases\n",
        "    self.last_totals = totals\n",
        "\n",
        "    exp = np.exp(totals)\n",
        "    return exp / np.sum(exp, axis=0)\n",
        "\n",
        "  def backprop(self, d_L_d_out, learn_rate):\n",
        "    '''\n",
        "    Performs a backward pass of the softmax layer.\n",
        "    Returns the loss gradient for this layer's inputs.\n",
        "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
        "    - learn_rate is a float.\n",
        "    '''\n",
        "    # We know only 1 element of d_L_d_out will be nonzero\n",
        "    for i, gradient in enumerate(d_L_d_out):\n",
        "      if gradient == 0:\n",
        "        continue\n",
        "\n",
        "      # e^totals\n",
        "      t_exp = np.exp(self.last_totals)\n",
        "\n",
        "      # Sum of all e^totals\n",
        "      S = np.sum(t_exp)\n",
        "\n",
        "      # Gradients of out[i] against totals\n",
        "      d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
        "      d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
        "\n",
        "      # Gradients of totals against weights/biases/input\n",
        "      d_t_d_w = self.last_input\n",
        "      d_t_d_b = 1\n",
        "      d_t_d_inputs = self.weights\n",
        "\n",
        "      # Gradients of loss against totals\n",
        "      d_L_d_t = gradient * d_out_d_t\n",
        "\n",
        "      # Gradients of loss against weights/biases/input\n",
        "      d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
        "      d_L_d_b = d_L_d_t * d_t_d_b\n",
        "      d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
        "\n",
        "      # Update weights / biases\n",
        "      self.weights -= learn_rate * d_L_d_w\n",
        "      self.biases -= learn_rate * d_L_d_b\n",
        "\n",
        "      return d_L_d_inputs.reshape(self.last_input_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YTQnTVVQWbo",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN4zJmeW4C3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "80ad6217-b767-42bd-bff6-2fb066793a46"
      },
      "source": [
        "conv = Convolution3x3(n_filters)\n",
        "pool = MaxPool2()\n",
        "softmax = Softmax((sz//2-1)*(sz//2-1)*n_filters, len(classes))\n",
        "\n",
        "\n",
        "\n",
        "def forward(image, label):\n",
        "  '''\n",
        "  Completes a forward pass of the CNN and calculates the accuracy and\n",
        "  cross-entropy loss.\n",
        "  - image is a 2d numpy array\n",
        "  - label is a digit\n",
        "  '''\n",
        "  # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
        "  # to work with. This is standard practice.\n",
        "  out = conv.forward((image / 255) - 0.5)\n",
        "  out = pool.forward(out)\n",
        "  out = softmax.forward(out)\n",
        "\n",
        "  # Calculate cross-entropy loss and accuracy. np.log() is the natural log.\n",
        "  loss = -np.log(out[label])\n",
        "  acc = 1 if np.argmax(out) == label else 0\n",
        "\n",
        "  return out, loss, acc\n",
        "\n",
        "def train(im, label, lr=.005):\n",
        "  '''\n",
        "  Completes a full training step on the given image and label.\n",
        "  Returns the cross-entropy loss and accuracy.\n",
        "  - image is a 2d numpy array\n",
        "  - label is a digit\n",
        "  - lr is the learning rate\n",
        "  '''\n",
        "  # Forward\n",
        "  out, loss, acc = forward(im, label)\n",
        "\n",
        "  # Calculate initial gradient\n",
        "  gradient = np.zeros(len(classes))\n",
        "  gradient[label] = -1 / out[label]\n",
        "\n",
        "  # Backprop\n",
        "  gradient = softmax.backprop(gradient, lr)\n",
        "  gradient = pool.backprop(gradient)\n",
        "  gradient = conv.backprop(gradient, lr)\n",
        "\n",
        "  return loss, acc\n",
        "\n",
        "\n",
        "# Train the CNN for 3 epochs\n",
        "for epoch in range(3):\n",
        "  print('--- Epoch %d ---' % (epoch + 1))\n",
        "\n",
        "  # Shuffle the training data\n",
        "  shuffle(data_train)\n",
        "\n",
        "  # Train!\n",
        "  loss = 0\n",
        "  num_correct = 0\n",
        "  for i, item in enumerate(data_train[:10000]):\n",
        "    if i > 0 and i % 100 == 99:\n",
        "      print(\n",
        "        '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
        "        (i + 1, loss / 100, num_correct)\n",
        "      )\n",
        "      loss = 0\n",
        "      num_correct = 0\n",
        "\n",
        "    l, acc = train(item[0], item[1])\n",
        "    loss += l\n",
        "    num_correct += acc\n"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Epoch 1 ---\n",
            "[Step 100] Past 100 steps: Average Loss 1.306 | Accuracy: 38%\n",
            "[Step 200] Past 100 steps: Average Loss 1.203 | Accuracy: 45%\n",
            "[Step 300] Past 100 steps: Average Loss 1.248 | Accuracy: 43%\n",
            "[Step 400] Past 100 steps: Average Loss 1.234 | Accuracy: 50%\n",
            "[Step 500] Past 100 steps: Average Loss 1.169 | Accuracy: 52%\n",
            "[Step 600] Past 100 steps: Average Loss 1.155 | Accuracy: 56%\n",
            "[Step 700] Past 100 steps: Average Loss 1.223 | Accuracy: 54%\n",
            "[Step 800] Past 100 steps: Average Loss 1.132 | Accuracy: 53%\n",
            "[Step 900] Past 100 steps: Average Loss 1.245 | Accuracy: 46%\n",
            "[Step 1000] Past 100 steps: Average Loss 1.147 | Accuracy: 52%\n",
            "[Step 1100] Past 100 steps: Average Loss 1.119 | Accuracy: 56%\n",
            "[Step 1200] Past 100 steps: Average Loss 0.990 | Accuracy: 57%\n",
            "[Step 1300] Past 100 steps: Average Loss 1.146 | Accuracy: 54%\n",
            "[Step 1400] Past 100 steps: Average Loss 1.121 | Accuracy: 54%\n",
            "[Step 1500] Past 100 steps: Average Loss 1.271 | Accuracy: 47%\n",
            "[Step 1600] Past 100 steps: Average Loss 1.110 | Accuracy: 59%\n",
            "[Step 1700] Past 100 steps: Average Loss 1.072 | Accuracy: 55%\n",
            "[Step 1800] Past 100 steps: Average Loss 1.092 | Accuracy: 53%\n",
            "[Step 1900] Past 100 steps: Average Loss 1.104 | Accuracy: 54%\n",
            "[Step 2000] Past 100 steps: Average Loss 1.118 | Accuracy: 54%\n",
            "[Step 2100] Past 100 steps: Average Loss 1.132 | Accuracy: 52%\n",
            "[Step 2200] Past 100 steps: Average Loss 1.170 | Accuracy: 54%\n",
            "[Step 2300] Past 100 steps: Average Loss 1.131 | Accuracy: 55%\n",
            "[Step 2400] Past 100 steps: Average Loss 1.095 | Accuracy: 51%\n",
            "[Step 2500] Past 100 steps: Average Loss 1.111 | Accuracy: 53%\n",
            "[Step 2600] Past 100 steps: Average Loss 1.140 | Accuracy: 51%\n",
            "[Step 2700] Past 100 steps: Average Loss 1.120 | Accuracy: 52%\n",
            "[Step 2800] Past 100 steps: Average Loss 1.027 | Accuracy: 59%\n",
            "[Step 2900] Past 100 steps: Average Loss 1.056 | Accuracy: 51%\n",
            "[Step 3000] Past 100 steps: Average Loss 1.074 | Accuracy: 56%\n",
            "[Step 3100] Past 100 steps: Average Loss 1.027 | Accuracy: 60%\n",
            "[Step 3200] Past 100 steps: Average Loss 0.978 | Accuracy: 63%\n",
            "[Step 3300] Past 100 steps: Average Loss 1.075 | Accuracy: 49%\n",
            "[Step 3400] Past 100 steps: Average Loss 1.107 | Accuracy: 58%\n",
            "[Step 3500] Past 100 steps: Average Loss 1.113 | Accuracy: 45%\n",
            "[Step 3600] Past 100 steps: Average Loss 0.945 | Accuracy: 57%\n",
            "[Step 3700] Past 100 steps: Average Loss 0.964 | Accuracy: 61%\n",
            "[Step 3800] Past 100 steps: Average Loss 1.097 | Accuracy: 56%\n",
            "[Step 3900] Past 100 steps: Average Loss 1.109 | Accuracy: 51%\n",
            "[Step 4000] Past 100 steps: Average Loss 1.162 | Accuracy: 53%\n",
            "[Step 4100] Past 100 steps: Average Loss 1.097 | Accuracy: 54%\n",
            "[Step 4200] Past 100 steps: Average Loss 0.941 | Accuracy: 58%\n",
            "[Step 4300] Past 100 steps: Average Loss 1.155 | Accuracy: 54%\n",
            "[Step 4400] Past 100 steps: Average Loss 1.140 | Accuracy: 56%\n",
            "[Step 4500] Past 100 steps: Average Loss 1.072 | Accuracy: 52%\n",
            "[Step 4600] Past 100 steps: Average Loss 1.076 | Accuracy: 56%\n",
            "[Step 4700] Past 100 steps: Average Loss 1.149 | Accuracy: 47%\n",
            "[Step 4800] Past 100 steps: Average Loss 1.108 | Accuracy: 54%\n",
            "[Step 4900] Past 100 steps: Average Loss 0.965 | Accuracy: 59%\n",
            "[Step 5000] Past 100 steps: Average Loss 1.118 | Accuracy: 56%\n",
            "[Step 5100] Past 100 steps: Average Loss 1.196 | Accuracy: 49%\n",
            "[Step 5200] Past 100 steps: Average Loss 0.988 | Accuracy: 60%\n",
            "[Step 5300] Past 100 steps: Average Loss 1.156 | Accuracy: 47%\n",
            "[Step 5400] Past 100 steps: Average Loss 0.923 | Accuracy: 61%\n",
            "[Step 5500] Past 100 steps: Average Loss 1.020 | Accuracy: 59%\n",
            "[Step 5600] Past 100 steps: Average Loss 0.980 | Accuracy: 57%\n",
            "[Step 5700] Past 100 steps: Average Loss 0.923 | Accuracy: 59%\n",
            "[Step 5800] Past 100 steps: Average Loss 1.122 | Accuracy: 52%\n",
            "[Step 5900] Past 100 steps: Average Loss 1.035 | Accuracy: 54%\n",
            "[Step 6000] Past 100 steps: Average Loss 1.139 | Accuracy: 54%\n",
            "[Step 6100] Past 100 steps: Average Loss 1.151 | Accuracy: 56%\n",
            "[Step 6200] Past 100 steps: Average Loss 1.020 | Accuracy: 56%\n",
            "[Step 6300] Past 100 steps: Average Loss 1.141 | Accuracy: 54%\n",
            "[Step 6400] Past 100 steps: Average Loss 1.053 | Accuracy: 58%\n",
            "[Step 6500] Past 100 steps: Average Loss 0.988 | Accuracy: 61%\n",
            "[Step 6600] Past 100 steps: Average Loss 0.925 | Accuracy: 67%\n",
            "[Step 6700] Past 100 steps: Average Loss 1.054 | Accuracy: 56%\n",
            "[Step 6800] Past 100 steps: Average Loss 0.983 | Accuracy: 63%\n",
            "[Step 6900] Past 100 steps: Average Loss 1.029 | Accuracy: 55%\n",
            "[Step 7000] Past 100 steps: Average Loss 1.065 | Accuracy: 60%\n",
            "[Step 7100] Past 100 steps: Average Loss 1.025 | Accuracy: 59%\n",
            "[Step 7200] Past 100 steps: Average Loss 1.144 | Accuracy: 47%\n",
            "[Step 7300] Past 100 steps: Average Loss 0.978 | Accuracy: 67%\n",
            "[Step 7400] Past 100 steps: Average Loss 1.190 | Accuracy: 47%\n",
            "[Step 7500] Past 100 steps: Average Loss 1.068 | Accuracy: 58%\n",
            "[Step 7600] Past 100 steps: Average Loss 1.145 | Accuracy: 55%\n",
            "[Step 7700] Past 100 steps: Average Loss 1.051 | Accuracy: 59%\n",
            "[Step 7800] Past 100 steps: Average Loss 1.107 | Accuracy: 52%\n",
            "[Step 7900] Past 100 steps: Average Loss 1.077 | Accuracy: 54%\n",
            "[Step 8000] Past 100 steps: Average Loss 0.944 | Accuracy: 63%\n",
            "[Step 8100] Past 100 steps: Average Loss 0.947 | Accuracy: 62%\n",
            "[Step 8200] Past 100 steps: Average Loss 0.987 | Accuracy: 57%\n",
            "[Step 8300] Past 100 steps: Average Loss 1.051 | Accuracy: 60%\n",
            "[Step 8400] Past 100 steps: Average Loss 0.830 | Accuracy: 73%\n",
            "[Step 8500] Past 100 steps: Average Loss 0.975 | Accuracy: 55%\n",
            "[Step 8600] Past 100 steps: Average Loss 0.882 | Accuracy: 63%\n",
            "[Step 8700] Past 100 steps: Average Loss 1.075 | Accuracy: 50%\n",
            "[Step 8800] Past 100 steps: Average Loss 1.043 | Accuracy: 55%\n",
            "[Step 8900] Past 100 steps: Average Loss 1.060 | Accuracy: 52%\n",
            "[Step 9000] Past 100 steps: Average Loss 0.925 | Accuracy: 63%\n",
            "[Step 9100] Past 100 steps: Average Loss 1.082 | Accuracy: 55%\n",
            "[Step 9200] Past 100 steps: Average Loss 0.891 | Accuracy: 62%\n",
            "[Step 9300] Past 100 steps: Average Loss 1.020 | Accuracy: 59%\n",
            "[Step 9400] Past 100 steps: Average Loss 0.893 | Accuracy: 64%\n",
            "[Step 9500] Past 100 steps: Average Loss 1.139 | Accuracy: 51%\n",
            "[Step 9600] Past 100 steps: Average Loss 1.095 | Accuracy: 54%\n",
            "[Step 9700] Past 100 steps: Average Loss 1.175 | Accuracy: 51%\n",
            "[Step 9800] Past 100 steps: Average Loss 0.925 | Accuracy: 64%\n",
            "[Step 9900] Past 100 steps: Average Loss 1.030 | Accuracy: 62%\n",
            "[Step 10000] Past 100 steps: Average Loss 0.962 | Accuracy: 61%\n",
            "--- Epoch 2 ---\n",
            "[Step 100] Past 100 steps: Average Loss 0.915 | Accuracy: 64%\n",
            "[Step 200] Past 100 steps: Average Loss 0.881 | Accuracy: 62%\n",
            "[Step 300] Past 100 steps: Average Loss 0.847 | Accuracy: 67%\n",
            "[Step 400] Past 100 steps: Average Loss 0.979 | Accuracy: 55%\n",
            "[Step 500] Past 100 steps: Average Loss 0.946 | Accuracy: 63%\n",
            "[Step 600] Past 100 steps: Average Loss 0.853 | Accuracy: 64%\n",
            "[Step 700] Past 100 steps: Average Loss 0.945 | Accuracy: 63%\n",
            "[Step 800] Past 100 steps: Average Loss 0.797 | Accuracy: 68%\n",
            "[Step 900] Past 100 steps: Average Loss 1.007 | Accuracy: 59%\n",
            "[Step 1000] Past 100 steps: Average Loss 0.925 | Accuracy: 60%\n",
            "[Step 1100] Past 100 steps: Average Loss 0.923 | Accuracy: 65%\n",
            "[Step 1200] Past 100 steps: Average Loss 0.946 | Accuracy: 65%\n",
            "[Step 1300] Past 100 steps: Average Loss 0.948 | Accuracy: 61%\n",
            "[Step 1400] Past 100 steps: Average Loss 0.899 | Accuracy: 65%\n",
            "[Step 1500] Past 100 steps: Average Loss 0.975 | Accuracy: 64%\n",
            "[Step 1600] Past 100 steps: Average Loss 0.938 | Accuracy: 62%\n",
            "[Step 1700] Past 100 steps: Average Loss 0.949 | Accuracy: 61%\n",
            "[Step 1800] Past 100 steps: Average Loss 1.071 | Accuracy: 53%\n",
            "[Step 1900] Past 100 steps: Average Loss 0.925 | Accuracy: 65%\n",
            "[Step 2000] Past 100 steps: Average Loss 0.941 | Accuracy: 57%\n",
            "[Step 2100] Past 100 steps: Average Loss 0.835 | Accuracy: 69%\n",
            "[Step 2200] Past 100 steps: Average Loss 0.842 | Accuracy: 71%\n",
            "[Step 2300] Past 100 steps: Average Loss 0.852 | Accuracy: 63%\n",
            "[Step 2400] Past 100 steps: Average Loss 0.843 | Accuracy: 64%\n",
            "[Step 2500] Past 100 steps: Average Loss 0.840 | Accuracy: 65%\n",
            "[Step 2600] Past 100 steps: Average Loss 0.990 | Accuracy: 58%\n",
            "[Step 2700] Past 100 steps: Average Loss 0.914 | Accuracy: 57%\n",
            "[Step 2800] Past 100 steps: Average Loss 0.859 | Accuracy: 62%\n",
            "[Step 2900] Past 100 steps: Average Loss 0.900 | Accuracy: 62%\n",
            "[Step 3000] Past 100 steps: Average Loss 0.995 | Accuracy: 61%\n",
            "[Step 3100] Past 100 steps: Average Loss 0.868 | Accuracy: 68%\n",
            "[Step 3200] Past 100 steps: Average Loss 0.839 | Accuracy: 65%\n",
            "[Step 3300] Past 100 steps: Average Loss 0.861 | Accuracy: 66%\n",
            "[Step 3400] Past 100 steps: Average Loss 0.877 | Accuracy: 64%\n",
            "[Step 3500] Past 100 steps: Average Loss 0.892 | Accuracy: 60%\n",
            "[Step 3600] Past 100 steps: Average Loss 1.013 | Accuracy: 57%\n",
            "[Step 3700] Past 100 steps: Average Loss 1.003 | Accuracy: 58%\n",
            "[Step 3800] Past 100 steps: Average Loss 1.065 | Accuracy: 58%\n",
            "[Step 3900] Past 100 steps: Average Loss 0.738 | Accuracy: 74%\n",
            "[Step 4000] Past 100 steps: Average Loss 0.984 | Accuracy: 64%\n",
            "[Step 4100] Past 100 steps: Average Loss 0.841 | Accuracy: 64%\n",
            "[Step 4200] Past 100 steps: Average Loss 0.889 | Accuracy: 63%\n",
            "[Step 4300] Past 100 steps: Average Loss 0.885 | Accuracy: 61%\n",
            "[Step 4400] Past 100 steps: Average Loss 0.971 | Accuracy: 61%\n",
            "[Step 4500] Past 100 steps: Average Loss 0.958 | Accuracy: 64%\n",
            "[Step 4600] Past 100 steps: Average Loss 0.975 | Accuracy: 60%\n",
            "[Step 4700] Past 100 steps: Average Loss 1.112 | Accuracy: 55%\n",
            "[Step 4800] Past 100 steps: Average Loss 0.955 | Accuracy: 62%\n",
            "[Step 4900] Past 100 steps: Average Loss 0.990 | Accuracy: 60%\n",
            "[Step 5000] Past 100 steps: Average Loss 1.057 | Accuracy: 56%\n",
            "[Step 5100] Past 100 steps: Average Loss 0.968 | Accuracy: 67%\n",
            "[Step 5200] Past 100 steps: Average Loss 0.889 | Accuracy: 60%\n",
            "[Step 5300] Past 100 steps: Average Loss 0.929 | Accuracy: 62%\n",
            "[Step 5400] Past 100 steps: Average Loss 1.085 | Accuracy: 55%\n",
            "[Step 5500] Past 100 steps: Average Loss 0.881 | Accuracy: 61%\n",
            "[Step 5600] Past 100 steps: Average Loss 0.989 | Accuracy: 59%\n",
            "[Step 5700] Past 100 steps: Average Loss 0.961 | Accuracy: 55%\n",
            "[Step 5800] Past 100 steps: Average Loss 0.798 | Accuracy: 70%\n",
            "[Step 5900] Past 100 steps: Average Loss 0.936 | Accuracy: 58%\n",
            "[Step 6000] Past 100 steps: Average Loss 0.978 | Accuracy: 60%\n",
            "[Step 6100] Past 100 steps: Average Loss 0.961 | Accuracy: 57%\n",
            "[Step 6200] Past 100 steps: Average Loss 1.037 | Accuracy: 59%\n",
            "[Step 6300] Past 100 steps: Average Loss 0.813 | Accuracy: 66%\n",
            "[Step 6400] Past 100 steps: Average Loss 0.978 | Accuracy: 61%\n",
            "[Step 6500] Past 100 steps: Average Loss 1.071 | Accuracy: 57%\n",
            "[Step 6600] Past 100 steps: Average Loss 0.818 | Accuracy: 68%\n",
            "[Step 6700] Past 100 steps: Average Loss 1.069 | Accuracy: 56%\n",
            "[Step 6800] Past 100 steps: Average Loss 0.926 | Accuracy: 58%\n",
            "[Step 6900] Past 100 steps: Average Loss 0.812 | Accuracy: 67%\n",
            "[Step 7000] Past 100 steps: Average Loss 1.059 | Accuracy: 58%\n",
            "[Step 7100] Past 100 steps: Average Loss 0.860 | Accuracy: 69%\n",
            "[Step 7200] Past 100 steps: Average Loss 0.832 | Accuracy: 67%\n",
            "[Step 7300] Past 100 steps: Average Loss 0.868 | Accuracy: 62%\n",
            "[Step 7400] Past 100 steps: Average Loss 0.906 | Accuracy: 62%\n",
            "[Step 7500] Past 100 steps: Average Loss 0.929 | Accuracy: 58%\n",
            "[Step 7600] Past 100 steps: Average Loss 1.126 | Accuracy: 52%\n",
            "[Step 7700] Past 100 steps: Average Loss 0.919 | Accuracy: 62%\n",
            "[Step 7800] Past 100 steps: Average Loss 0.865 | Accuracy: 64%\n",
            "[Step 7900] Past 100 steps: Average Loss 0.904 | Accuracy: 68%\n",
            "[Step 8000] Past 100 steps: Average Loss 0.836 | Accuracy: 62%\n",
            "[Step 8100] Past 100 steps: Average Loss 1.018 | Accuracy: 55%\n",
            "[Step 8200] Past 100 steps: Average Loss 0.890 | Accuracy: 63%\n",
            "[Step 8300] Past 100 steps: Average Loss 0.929 | Accuracy: 60%\n",
            "[Step 8400] Past 100 steps: Average Loss 1.024 | Accuracy: 60%\n",
            "[Step 8500] Past 100 steps: Average Loss 0.928 | Accuracy: 64%\n",
            "[Step 8600] Past 100 steps: Average Loss 0.936 | Accuracy: 59%\n",
            "[Step 8700] Past 100 steps: Average Loss 1.036 | Accuracy: 58%\n",
            "[Step 8800] Past 100 steps: Average Loss 0.936 | Accuracy: 60%\n",
            "[Step 8900] Past 100 steps: Average Loss 0.956 | Accuracy: 63%\n",
            "[Step 9000] Past 100 steps: Average Loss 1.089 | Accuracy: 59%\n",
            "[Step 9100] Past 100 steps: Average Loss 0.942 | Accuracy: 57%\n",
            "[Step 9200] Past 100 steps: Average Loss 1.047 | Accuracy: 55%\n",
            "[Step 9300] Past 100 steps: Average Loss 1.065 | Accuracy: 53%\n",
            "[Step 9400] Past 100 steps: Average Loss 0.981 | Accuracy: 62%\n",
            "[Step 9500] Past 100 steps: Average Loss 0.954 | Accuracy: 64%\n",
            "[Step 9600] Past 100 steps: Average Loss 1.089 | Accuracy: 54%\n",
            "[Step 9700] Past 100 steps: Average Loss 1.048 | Accuracy: 54%\n",
            "[Step 9800] Past 100 steps: Average Loss 1.020 | Accuracy: 55%\n",
            "[Step 9900] Past 100 steps: Average Loss 1.086 | Accuracy: 57%\n",
            "[Step 10000] Past 100 steps: Average Loss 1.117 | Accuracy: 60%\n",
            "--- Epoch 3 ---\n",
            "[Step 100] Past 100 steps: Average Loss 0.878 | Accuracy: 63%\n",
            "[Step 200] Past 100 steps: Average Loss 1.000 | Accuracy: 53%\n",
            "[Step 300] Past 100 steps: Average Loss 0.718 | Accuracy: 72%\n",
            "[Step 400] Past 100 steps: Average Loss 0.960 | Accuracy: 59%\n",
            "[Step 500] Past 100 steps: Average Loss 0.805 | Accuracy: 66%\n",
            "[Step 600] Past 100 steps: Average Loss 0.805 | Accuracy: 68%\n",
            "[Step 700] Past 100 steps: Average Loss 0.860 | Accuracy: 66%\n",
            "[Step 800] Past 100 steps: Average Loss 0.836 | Accuracy: 70%\n",
            "[Step 900] Past 100 steps: Average Loss 0.860 | Accuracy: 59%\n",
            "[Step 1000] Past 100 steps: Average Loss 0.772 | Accuracy: 68%\n",
            "[Step 1100] Past 100 steps: Average Loss 0.899 | Accuracy: 63%\n",
            "[Step 1200] Past 100 steps: Average Loss 1.111 | Accuracy: 57%\n",
            "[Step 1300] Past 100 steps: Average Loss 0.795 | Accuracy: 64%\n",
            "[Step 1400] Past 100 steps: Average Loss 0.676 | Accuracy: 70%\n",
            "[Step 1500] Past 100 steps: Average Loss 0.797 | Accuracy: 70%\n",
            "[Step 1600] Past 100 steps: Average Loss 0.879 | Accuracy: 61%\n",
            "[Step 1700] Past 100 steps: Average Loss 0.895 | Accuracy: 70%\n",
            "[Step 1800] Past 100 steps: Average Loss 0.921 | Accuracy: 63%\n",
            "[Step 1900] Past 100 steps: Average Loss 0.933 | Accuracy: 61%\n",
            "[Step 2000] Past 100 steps: Average Loss 0.855 | Accuracy: 64%\n",
            "[Step 2100] Past 100 steps: Average Loss 0.746 | Accuracy: 71%\n",
            "[Step 2200] Past 100 steps: Average Loss 0.825 | Accuracy: 70%\n",
            "[Step 2300] Past 100 steps: Average Loss 0.783 | Accuracy: 72%\n",
            "[Step 2400] Past 100 steps: Average Loss 0.789 | Accuracy: 67%\n",
            "[Step 2500] Past 100 steps: Average Loss 0.762 | Accuracy: 70%\n",
            "[Step 2600] Past 100 steps: Average Loss 0.972 | Accuracy: 58%\n",
            "[Step 2700] Past 100 steps: Average Loss 0.825 | Accuracy: 58%\n",
            "[Step 2800] Past 100 steps: Average Loss 0.796 | Accuracy: 70%\n",
            "[Step 2900] Past 100 steps: Average Loss 0.847 | Accuracy: 67%\n",
            "[Step 3000] Past 100 steps: Average Loss 0.964 | Accuracy: 58%\n",
            "[Step 3100] Past 100 steps: Average Loss 0.876 | Accuracy: 63%\n",
            "[Step 3200] Past 100 steps: Average Loss 0.694 | Accuracy: 73%\n",
            "[Step 3300] Past 100 steps: Average Loss 0.885 | Accuracy: 63%\n",
            "[Step 3400] Past 100 steps: Average Loss 0.874 | Accuracy: 67%\n",
            "[Step 3500] Past 100 steps: Average Loss 0.815 | Accuracy: 68%\n",
            "[Step 3600] Past 100 steps: Average Loss 0.752 | Accuracy: 67%\n",
            "[Step 3700] Past 100 steps: Average Loss 1.024 | Accuracy: 56%\n",
            "[Step 3800] Past 100 steps: Average Loss 0.909 | Accuracy: 59%\n",
            "[Step 3900] Past 100 steps: Average Loss 0.833 | Accuracy: 70%\n",
            "[Step 4000] Past 100 steps: Average Loss 0.911 | Accuracy: 65%\n",
            "[Step 4100] Past 100 steps: Average Loss 0.938 | Accuracy: 67%\n",
            "[Step 4200] Past 100 steps: Average Loss 0.846 | Accuracy: 66%\n",
            "[Step 4300] Past 100 steps: Average Loss 0.940 | Accuracy: 60%\n",
            "[Step 4400] Past 100 steps: Average Loss 0.799 | Accuracy: 65%\n",
            "[Step 4500] Past 100 steps: Average Loss 1.073 | Accuracy: 48%\n",
            "[Step 4600] Past 100 steps: Average Loss 0.913 | Accuracy: 62%\n",
            "[Step 4700] Past 100 steps: Average Loss 0.852 | Accuracy: 65%\n",
            "[Step 4800] Past 100 steps: Average Loss 0.882 | Accuracy: 70%\n",
            "[Step 4900] Past 100 steps: Average Loss 0.853 | Accuracy: 66%\n",
            "[Step 5000] Past 100 steps: Average Loss 0.925 | Accuracy: 66%\n",
            "[Step 5100] Past 100 steps: Average Loss 0.786 | Accuracy: 64%\n",
            "[Step 5200] Past 100 steps: Average Loss 0.892 | Accuracy: 64%\n",
            "[Step 5300] Past 100 steps: Average Loss 0.853 | Accuracy: 60%\n",
            "[Step 5400] Past 100 steps: Average Loss 1.035 | Accuracy: 57%\n",
            "[Step 5500] Past 100 steps: Average Loss 0.770 | Accuracy: 68%\n",
            "[Step 5600] Past 100 steps: Average Loss 0.882 | Accuracy: 66%\n",
            "[Step 5700] Past 100 steps: Average Loss 0.886 | Accuracy: 66%\n",
            "[Step 5800] Past 100 steps: Average Loss 0.729 | Accuracy: 74%\n",
            "[Step 5900] Past 100 steps: Average Loss 0.856 | Accuracy: 67%\n",
            "[Step 6000] Past 100 steps: Average Loss 1.076 | Accuracy: 56%\n",
            "[Step 6100] Past 100 steps: Average Loss 0.760 | Accuracy: 75%\n",
            "[Step 6200] Past 100 steps: Average Loss 0.935 | Accuracy: 63%\n",
            "[Step 6300] Past 100 steps: Average Loss 0.870 | Accuracy: 67%\n",
            "[Step 6400] Past 100 steps: Average Loss 0.958 | Accuracy: 63%\n",
            "[Step 6500] Past 100 steps: Average Loss 0.663 | Accuracy: 73%\n",
            "[Step 6600] Past 100 steps: Average Loss 0.920 | Accuracy: 62%\n",
            "[Step 6700] Past 100 steps: Average Loss 0.976 | Accuracy: 58%\n",
            "[Step 6800] Past 100 steps: Average Loss 0.859 | Accuracy: 67%\n",
            "[Step 6900] Past 100 steps: Average Loss 0.792 | Accuracy: 68%\n",
            "[Step 7000] Past 100 steps: Average Loss 0.811 | Accuracy: 67%\n",
            "[Step 7100] Past 100 steps: Average Loss 1.020 | Accuracy: 63%\n",
            "[Step 7200] Past 100 steps: Average Loss 0.856 | Accuracy: 68%\n",
            "[Step 7300] Past 100 steps: Average Loss 0.777 | Accuracy: 73%\n",
            "[Step 7400] Past 100 steps: Average Loss 0.973 | Accuracy: 64%\n",
            "[Step 7500] Past 100 steps: Average Loss 0.821 | Accuracy: 64%\n",
            "[Step 7600] Past 100 steps: Average Loss 0.895 | Accuracy: 68%\n",
            "[Step 7700] Past 100 steps: Average Loss 1.004 | Accuracy: 60%\n",
            "[Step 7800] Past 100 steps: Average Loss 0.827 | Accuracy: 72%\n",
            "[Step 7900] Past 100 steps: Average Loss 0.901 | Accuracy: 60%\n",
            "[Step 8000] Past 100 steps: Average Loss 0.823 | Accuracy: 69%\n",
            "[Step 8100] Past 100 steps: Average Loss 1.010 | Accuracy: 59%\n",
            "[Step 8200] Past 100 steps: Average Loss 0.829 | Accuracy: 71%\n",
            "[Step 8300] Past 100 steps: Average Loss 0.870 | Accuracy: 61%\n",
            "[Step 8400] Past 100 steps: Average Loss 0.777 | Accuracy: 71%\n",
            "[Step 8500] Past 100 steps: Average Loss 0.937 | Accuracy: 59%\n",
            "[Step 8600] Past 100 steps: Average Loss 0.829 | Accuracy: 64%\n",
            "[Step 8700] Past 100 steps: Average Loss 1.002 | Accuracy: 59%\n",
            "[Step 8800] Past 100 steps: Average Loss 0.948 | Accuracy: 58%\n",
            "[Step 8900] Past 100 steps: Average Loss 0.807 | Accuracy: 67%\n",
            "[Step 9000] Past 100 steps: Average Loss 0.758 | Accuracy: 68%\n",
            "[Step 9100] Past 100 steps: Average Loss 1.029 | Accuracy: 56%\n",
            "[Step 9200] Past 100 steps: Average Loss 0.950 | Accuracy: 56%\n",
            "[Step 9300] Past 100 steps: Average Loss 0.865 | Accuracy: 64%\n",
            "[Step 9400] Past 100 steps: Average Loss 0.884 | Accuracy: 63%\n",
            "[Step 9500] Past 100 steps: Average Loss 0.887 | Accuracy: 64%\n",
            "[Step 9600] Past 100 steps: Average Loss 0.911 | Accuracy: 59%\n",
            "[Step 9700] Past 100 steps: Average Loss 0.955 | Accuracy: 66%\n",
            "[Step 9800] Past 100 steps: Average Loss 0.970 | Accuracy: 63%\n",
            "[Step 9900] Past 100 steps: Average Loss 1.023 | Accuracy: 64%\n",
            "[Step 10000] Past 100 steps: Average Loss 0.931 | Accuracy: 66%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS7CVxsMQaBv",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt8Rmqr6qUst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "890c9db0-90f1-4c5b-d92e-f20417254e22"
      },
      "source": [
        "# Test the CNN\n",
        "print('\\n--- Testing the CNN ---')\n",
        "loss = 0\n",
        "num_correct = 0\n",
        "for item in data_test[:500]:\n",
        "  _, l, acc = forward(item[0], item[1])\n",
        "  loss += l\n",
        "  num_correct += acc\n",
        "\n",
        "num_tests = len(data_test[:500])\n",
        "print('Test Loss:', loss / num_tests)\n",
        "print('Test Accuracy:', num_correct / num_tests)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--- Testing the CNN ---\n",
            "Test Loss: 0.9759738998987465\n",
            "Test Accuracy: 0.636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8WXC-f-rnWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "d0139dc0-df6b-4b38-f470-7c6deffa2f24"
      },
      "source": [
        "for j in range(2):\n",
        "  index = randint(0, len(data_test))\n",
        "  image, label = data_test[index]\n",
        "  plt.figure()\n",
        "  plt.imshow(image, cmap='gray')\n",
        "  plt.show()\n",
        "  print(\"Actual class: \", classes[label])\n",
        "  pred, l, acc = forward(image, label)\n",
        "  print(\"Predicted class: \", classes[np.argmax(pred)])\n",
        "  print(\"---------\")"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Actual class:  grass\n",
            "Predicted class:  grass\n",
            "---------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8B2ngPjAd5wC4gPfcQqID3zHKSD/B47c+lwXEYavAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Actual class:  soybean\n",
            "Predicted class:  soybean\n",
            "---------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}